{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.12'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'I am learning how to build chatbots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "am VERB\n",
      "learning VERB\n",
      "how ADV\n",
      "to PART\n",
      "build VERB\n",
      "chatbots NOUN\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u'I am going to London next weekend for a meeting') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "am VERB\n",
      "going VERB\n",
      "to ADP\n",
      "London PROPN\n",
      "next ADJ\n",
      "weekend NOUN\n",
      "for ADP\n",
      "a DET\n",
      "meeting NOUN\n"
     ]
    }
   ],
   "source": [
    "for token in doc2:\n",
    "    print(token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc is a container object for accessing annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp(u'Google release \"Move mirror\" AI experiment that matches your pose from 80,000 images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google google PROPN NNP compound Xxxxx True False\n",
      "release release NOUN NN nmod xxxx True False\n",
      "\" \" PUNCT `` punct \" False False\n",
      "Move move NOUN NN nmod Xxxx True False\n",
      "mirror mirror NOUN NN nmod xxxx True False\n",
      "\" \" PUNCT '' punct \" False False\n",
      "AI ai PROPN NNP compound XX True False\n",
      "experiment experiment NOUN NN ROOT xxxx True False\n",
      "that that ADJ WDT nsubj xxxx True True\n",
      "matches match VERB VBZ relcl xxxx True False\n",
      "your -PRON- ADJ PRP$ poss xxxx True True\n",
      "pose pose NOUN NN dobj xxxx True False\n",
      "from from ADP IN prep xxxx True True\n",
      "80,000 80,000 NUM CD nummod dd,ddd False False\n",
      "images image NOUN NNS pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "for token in doc3:\n",
    "    print(token.text,token.lemma_,token.pos_,token.tag_,token.dep_,token.shape_,token.is_alpha,token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-19-f1a96bc06c43>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-f1a96bc06c43>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    for token in doc:\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "for token in doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -PRON- PRON PRP nsubj X True False\n",
      "am be VERB VBP aux xx True True\n",
      "going go VERB VBG ROOT xxxx True False\n",
      "to to ADP IN prep xx True True\n",
      "London london PROPN NNP pobj Xxxxx True False\n",
      "next next ADJ JJ amod xxxx True True\n",
      "week week NOUN NN npadvmod xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "a a DET DT det x True True\n",
      "meeting meeting NOUN NN pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text,token.lemma_,token.pos_,token.tag_,token.dep_,token.shape_,token.is_alpha,token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEMMA Root form of the word being processed\n",
    "POS Part-of-speech of the word\n",
    "TAG They express the part-of-speech (e.g., VERB) and some amount of morphological\n",
    "information (e.g., that the verb is past tense).\n",
    "DEP Syntactic dependency (i.e., the relation between tokens)\n",
    "SHAPE Shape of the word (e.g., the capitalization, punctuation, digits format)\n",
    "ALPHA Is the token an alpha character?\n",
    "Stop Is the word a stop word or part of a stop list?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEED FOR POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: to reduce the complexity of understanding a text that can’t be trained or\n",
    "is trained with less confidence. By use of POS tagging, we can identify parts of the text\n",
    "input and do string matching only for those parts. For example, if you were to find if a\n",
    "location exists in a sentence, then POS tagging would tag the location word as NOUN, so\n",
    "you can take all the NOUNs from the tagged list and see if it’s one of the locations from\n",
    "your preset list or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEMMING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing inflected words to their word stem, base form.\n",
    "A stemming algorithm reduces the words “saying” to the root word “say,” whereas\n",
    "“presumably” becomes presum. As you can see, this may or may not always be 100%\n",
    "correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Lemmatization tries to do the job more elegantly with the use of a\n",
    "vocabulary and morphological analysis of words. It tries its best to\n",
    "remove inflectional endings only and return the dictionary form of a\n",
    "word, known as the lemma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Stemming does the job in a crude, heuristic way that chops off the\n",
    "ends of words, assuming that the remaining word is what we are\n",
    "actually looking for, but it often includes the removal of derivational\n",
    "affixes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chuckle']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX ,LEMMA_EXC, LEMMA_RULES\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC,LEMMA_RULES)\n",
    "lemmatizer('chuckles','NOUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blaze']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('blazing','verb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fast']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('fastest','adj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE OF STEMMERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using PorterStemmer And SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastest\n",
      "fastest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "print(porter_stemmer.stem('fastest'))\n",
    "print(snowball_stemmer.stem('fastest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE OF LEMMATIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must have wondered how Google gives you\n",
    "the articles in search results that you meant to get even when the search text was not\n",
    "properly formulated.\n",
    "This is where one makes use of lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named-Entity-Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string= u\"Google has it's headquarters in Mountain View,California having revenue amounted to 109.65 billion US dollars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google ORG\n",
      "Mountain View GPE\n",
      "California GPE\n",
      "109.65 billion US dollars MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string2 = u\"Mark Zuckerberg born May 14,1984 in New York is an American  technology entrpreneur and philanthropist best known for co-founding and leading Facebook as it's chairman and CEO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1= nlp(my_string2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark Zuckerberg PERSON\n",
      "May 14,1984 DATE\n",
      "New York GPE\n",
      "American NORP\n",
      "Facebook ORG\n"
     ]
    }
   ],
   "source": [
    "for ent in doc1.ents:\n",
    "    print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3=nlp(u\"I usually wakke up at 9:00 AM . 90% of my daytime goes in learning new things.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9:00 AM TIME\n",
      "90% PERCENT\n"
     ]
    }
   ],
   "source": [
    "for ent in doc3.ents:\n",
    "    print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the entity extractor can easily extract the time information from the\n",
    "given string. Also as you can see entity extractor not just tries to identify the number but\n",
    "also exact PERCENTAGE value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 =  nlp(u\"Imagine Dragons are the best band\")\n",
    "doc5 =  nlp(u\"Imagine Dragons come and take over the city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine Dragons ORG\n"
     ]
    }
   ],
   "source": [
    "for ent in doc4.ents:\n",
    "    print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc5.ents:\n",
    "    print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine you were to extract the context of the above two strings in a live\n",
    "environment. What would you do? With help of Entity Extractor, one can easily figure out\n",
    "the context of the statement and intelligently take the conversation further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are high-frequency words like a, an, the, to and also that we sometimes\n",
    "want to filter out of a document before further processing. Stop words usually have little\n",
    "lexical content and do not hold much of a meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the words which are identified as stopwords in spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yours', 'call', 'make', 'ten', 'at', 'any', 'even', 'after', 'indeed', 'most', 'something', 'sometimes', 'much', 'somewhere', 'me', 'whole', 'will', 'yourselves', 'already', 'again', 'with', 'afterwards', 'give', 'her', 'besides', 'name', 'become', 'becoming', 'down', 'first', 'least', 'only', 'is', 'thus', 'must', 'none', 'third', 'whence', 'beyond', 'herein', 'fifteen', 'there', 'be', 'enough', 'move', 'unless', 'every', 'along', 'five', 'own', 'hereafter', 'he', 'hereupon', 'just', 'thence', 'ca', 'whereas', 'made', 'within', 'before', 'formerly', 'them', 'one', 'these', 'whither', 'however', 'when', 'they', 'a', 'various', 'bottom', 'wherein', 'alone', 'was', 'that', 'serious', 'well', 'almost', 'go', 'nowhere', 'once', 'mostly', 'should', 'whenever', 'nothing', 'does', 'whereafter', 'ourselves', 'beforehand', 'several', 'upon', 'such', 'others', 'keep', 'had', 'over', 'out', 'either', 'each', 'may', 'themselves', 'i', 'twelve', 'nevertheless', 'has', 'namely', 'amount', 'else', 'thru', 'twenty', 'moreover', 'up', 'through', 'been', 'became', 'everywhere', 'not', 'many', 'but', 'on', 'between', 'rather', 'still', 'my', 'becomes', 'among', 'above', 'back', 'itself', 'since', 'ever', 'six', 'whereupon', 'four', 'empty', 'than', 'three', 'eleven', 'why', 'our', 'while', 'hundred', 'per', 'across', 'towards', 'wherever', 'we', 'have', 'somehow', 'it', 'or', 'both', 'herself', 'seems', 'so', 'side', 'where', 'hers', 'two', 'all', 'part', 'his', 'forty', 'say', 'whose', 'those', 'himself', 'whom', 'you', 'sometime', 'less', 'yourself', 'seem', 'nor', 'put', 'would', 'to', 'now', 'onto', 'seeming', 'together', 'used', 'because', 'its', 'eight', 'show', 'take', 'anyone', 'in', 'into', 'how', 'other', 'what', 'your', 'everyone', 'here', 'more', 'whereby', 'fifty', 'thereafter', 'off', 'do', 'elsewhere', 'were', 'without', 'did', 'noone', 'quite', 'this', 'might', 'and', 'are', 'due', 'if', 'an', 'another', 'as', 'done', 'full', 'last', 'nine', 'below', 'she', 'the', 'around', 'using', 'hereby', 'meanwhile', 'often', 'get', 'therefore', 'front', 'anyhow', 'until', 'though', 'thereby', 'see', 'please', 'anyway', 'who', 'neither', 'otherwise', 'same', 'although', 'could', 'seemed', 'thereupon', 'being', 'mine', 'of', 'cannot', 'am', 'therein', 'few', 're', 'then', 'always', 'anything', 'former', 'their', 'very', 'top', 'nobody', 'whether', 'yet', 'him', 'sixty', 'some', 'also', 'about', 'latter', 'us', 'via', 'whoever', 'whatever', 'everything', 'myself', 'behind', 'beside', 'can', 'doing', 'perhaps', 'someone', 'from', 'further', 'no', 'anywhere', 'hence', 'throughout', 'for', 'too', 'under', 'against', 'during', 'next', 'amongst', 'really', 'which', 'latterly', 'ours', 'regarding', 'never', 'except', 'toward', 'by'}\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are about 305 stop words defined in spaCy’s stop words list. You can always\n",
    "define your own stop words if needed and override the existing list.\n",
    "To see if a word is a stop word or not, you can use the nlp object of spaCy. We can use\n",
    "the nlp object’s is_stop attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[u'is'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[u\"hello\"].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[u'with'].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are a very important part of text clean up. It helps removal of meaningless\n",
    "data before we try to do actual processing to make sense of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc6 = nlp(u'Book me a flight from Bangalore to Goa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "blr,goa = doc6[5],doc6[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bangalore"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc6[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Goa"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc6[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[from, flight, Book]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(blr.ancestors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[to, flight, Book]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(goa.ancestors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Are Ancestors in Dependency Parsing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ancestors are the rightmost token of this token’s syntactic descendants. Like in the above\n",
    "example for the object blr the ancestors were from, flight, and Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to check whether a particular element is ancestor or not\n",
    "doc6[3].is_ancestor(doc6[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "docf = nlp('I want to book a cab to the hotel and a table at a restaurant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-67-f17af37d47c4>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-67-f17af37d47c4>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    as = docf[8]\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "as = docf[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[to, book, want]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(docf[8].ancestors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Booking of Table belongs to restaurant\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Book a Table at the restaurant and the taxi to the hotel')\n",
    "tasks = doc[2],doc[8]  #(table,taxi)\n",
    "tasks_target = doc[5],doc[11]  #(restaurant,hotel)\n",
    "\n",
    "for task in tasks_target:\n",
    "    for tok in task.ancestors:\n",
    "        if tok in tasks:\n",
    "            print('Booking of {} belongs to {}'.format(tok,task))\n",
    "            \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Visualization for Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'encoding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-506b19a6b9c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdisplacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'Book a table at the restaurant and the taxi to the hotel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdisplacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dep'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#go to the link in your browser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# http://localhost:5000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\spacy\\displacy\\__init__.py\u001b[0m in \u001b[0;36mserve\u001b[1;34m(docs, style, page, minify, options, manual, port)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mwsgiref\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msimple_server\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     render(docs, style=style, page=page, minify=minify, options=options,\n\u001b[1;32m---> 62\u001b[1;33m            manual=manual)\n\u001b[0m\u001b[0;32m     63\u001b[0m     \u001b[0mhttpd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimple_server\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_server\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'0.0.0.0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     prints(\"Using the '{}' visualizer\".format(style),\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\spacy\\displacy\\__init__.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(docs, style, page, minify, jupyter, options, manual)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfactories\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mrenderer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mparsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmanual\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0m_html\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'parsed'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mminify\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_html\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'parsed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\spacy\\displacy\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfactories\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mrenderer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mparsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmanual\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0m_html\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'parsed'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mminify\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_html\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'parsed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\spacy\\displacy\\__init__.py\u001b[0m in \u001b[0;36mparse_deps\u001b[1;34m(orig_doc, options)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mGenerated\u001b[0m \u001b[0mdependency\u001b[0m \u001b[0mparse\u001b[0m \u001b[0mkeyed\u001b[0m \u001b[0mby\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marcs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \"\"\"\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_doc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_doc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0muser_warning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW005\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.to_bytes\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mto_bytes\u001b[1;34m(getters, exclude)\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m             \u001b[0mserialized\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmsgpack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_bin_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\msgpack_numpy.py\u001b[0m in \u001b[0;36mpackb\u001b[1;34m(o, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m     \"\"\"\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mPacker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0munpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'encoding'"
     ]
    }
   ],
   "source": [
    "#run the following code\n",
    "from spacy import displacy\n",
    "doc = nlp(u'Book a table at the restaurant and the taxi to the hotel')\n",
    "displacy.serve(doc, style='dep')\n",
    "#go to the link in your browser\n",
    "# http://localhost:5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'What are some places to visit in Berlin and stay in Lubeck')\n",
    "places = [doc[7],doc[11]]\n",
    "actions = [doc[5],doc[9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User is referring Berlin to visit\n",
      "User is referring Lubeck to stay\n"
     ]
    }
   ],
   "source": [
    "for place in places:\n",
    "    for tok in place.ancestors:\n",
    "        if tok in actions:\n",
    "            print('User is referring {} to {}'.format(place,tok))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see in these examples, dependency parsing makes it quite easy to understand\n",
    "what the user is referring to. We saw that in the case of two different tasks as well, we can\n",
    "pretty figure out the expectation and, based on that, formulate the next response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing Helps in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• It helps in finding relationships between words of grammatically\n",
    "correct sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• It can be used for sentence boundary detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• It is quite useful to find out if the user is talking about more than one\n",
    "context simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'Boston Dynamics is gearing up to produce thousands of robot dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Boston Dynamics, thousands, robot dogs]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though having noun chunks from a given sentence helps a lot, spaCy provides other\n",
    "attributes that can be helpful too. Let’s try to explore some of those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"Deep learning cracks the code of messsenger RNAs and protein coding potential\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning learning nsubj cracks\n",
      "the code code dobj cracks\n",
      "messsenger RNAs RNAs pobj of\n",
      "protein protein conj RNAs\n",
      "potential potential dobj coding\n"
     ]
    }
   ],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text,chunk.root.text,chunk.root.dep_,chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for\n",
    "words. GloVe algorithm uses aggregated global word-word co-occurrence statistics from\n",
    "a corpus to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'How are you doing today?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How [-0.29742572  0.73939663 -0.04001489  0.44033998  2.8967493 ]\n",
      "are [-0.23435052 -1.6145046   1.0197451   0.99281657  0.28227103]\n",
      "you [ 0.10252154 -3.5647116   2.482279    4.2825      3.5902457 ]\n",
      "doing [-0.6240918 -2.0210214 -0.9101492  2.7051926  4.189254 ]\n",
      "today [ 3.5409102  -0.62185943  2.6274269   2.050488    0.20191962]\n",
      "? [ 2.8915     -0.25079128  3.3764174   1.6942688   1.9849055 ]\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text,token.vector[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a 5D Vector used for the representation of a particular word in the vector domain for cosine similarity or to find similarity between two different words under consideration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Example-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello1 = nlp(u'Hello')\n",
    "hi_doc = nlp(u'hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7522951892083747\n",
      "0.38901935595436765\n"
     ]
    }
   ],
   "source": [
    "hella_doc = nlp(u'hella')\n",
    "print(hello1.similarity(hi_doc))\n",
    "print(hello1.similarity(hella_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between two sentences is 0.7063789688496691\n"
     ]
    }
   ],
   "source": [
    "str1 = nlp(u'When will next season of Game of Thrones be releasing?')\n",
    "str2 = nlp(u'Game of thrones next season release date?')\n",
    "sim_str = str1.similarity(str2)\n",
    "print('The similarity between two sentences is {}'.format(sim_str))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As we can see in this example, the similarity between both of the sentences is about\n",
    "70%, which is good enough to say that both of the sentences are quite similar, which is\n",
    "true. This can help us save a lot of time for writing custom code when building chatbots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_doc = nlp(u'car truck google')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word car is 100% similar to word car\n",
      "Word car is 71% similar to word truck\n",
      "Word car is 24% similar to word google\n",
      "Word truck is 71% similar to word car\n",
      "Word truck is 100% similar to word truck\n",
      "Word truck is 36% similar to word google\n",
      "Word google is 24% similar to word car\n",
      "Word google is 36% similar to word truck\n",
      "Word google is 100% similar to word google\n"
     ]
    }
   ],
   "source": [
    "for t1 in example_doc:\n",
    "    for t2 in example_doc:\n",
    "        similarity_perc = int(t1.similarity(t2)*100)\n",
    "        print(\"Word {} is {}% similar to word {}\".format(t1.text,similarity_perc, t2.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding similarity between words or sentences becomes quite important when we\n",
    "intend to build any application that is hugely dependent on the implementations of\n",
    "NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is one of the simple yet basic concepts of NLP where we split a text into\n",
    "meaningful segments. spaCy first tokenizes the text (i.e., segments it into words and\n",
    "then punctuation and other things)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brexit\n",
      "is\n",
      "the\n",
      "impending\n",
      "withdrawal\n",
      "of\n",
      "U.K.\n",
      "from\n",
      "the\n",
      "European\n",
      "Union\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Brexit is the impending withdrawal of U.K. from the European Union. ')\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see in the above output, U.K. comes as a single word after the tokenization\n",
    "process, which makes sense, as U.K. is a country name and splitting it would be wrong.\n",
    "Even after this if you not happy with spaCy’s tokenization, then you can use its\n",
    "add_special_case case method to add your own rule before relying completely on\n",
    "spaCy’s tokenization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sent1 = \"Book me a metro from Airport Station to Hong Kong Station\"\n",
    "sent2 = \"Book me a cab to Honk Kong Airport from AsiaWorld-Expo.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
